{
"metadata": {
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": 3
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.7.2"
 }
},
"nbformat": 4,
"nbformat_minor": 2, 
"cells": [
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"import matplotlib.pyplot as plt\n",
"import numpy as np\n",
"\n",
"import tensorflow.keras as keras\n",
"import tensorflow.keras.layers as layers\n",
"import tensorflow.keras.models as models\n",
"import tensorflow.keras.backend as K\n",
"\n",
"from tensorflow.keras.datasets import fashion_mnist\n",
"(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
"\n",
"image_size = x_train.shape[1]\n",
"original_dim = image_size * image_size\n",
"x_train = x_train.reshape([-1, original_dim]).astype('float32')/255.0\n",
"x_test = x_test.reshape([-1, original_dim]).astype('float32')/255.0\n",
"\n",
"batch_size = 100\n",
"intermediate_dim = 512\n",
"encoded_dim = 20\n",
"epochs = 5"]},{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"def sample_normal(args):\n",
"    "Sample from the normal distribution with given mean and variance"\n",
"    mean, logsigma = args\n",
"    batch, dim = K.shape(mean)[0], K.int_shape(mean)[1]\n",
"    Z = K.random_normal(shape=(batch, dim))\n",
"    return mean + K.exp(0.5 * logsigma) * Z\n",
"\n",
"def vae_loss(x, x_decoded):\n",
"    "Return cross-entropy loss plus divergence of encoded layer from standard normal"\n",
"    crossentropy_loss = keras.losses.binary_crossentropy(x, x_decoded)\n",
"    kl_loss = - 0.5 * K.mean(1 + logsigma - K.square(mean) - K.exp(logsigma), axis=-1)\n",
"    return crossentropy_loss + kl_loss "]},{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"x = layers.Input(shape=(original_dim,))\n",
"h = layers.Dense(intermediate_dim, activation='relu')(x) # hidden encoding layer\n",
"mean = layers.Dense(encoded_dim)(h) # μ(x) layer\n",
"logsigma = layers.Dense(encoded_dim)(h) # Λ(x) layer\n",
"sample = layers.Lambda(sample_normal)([mean, logsigma]) # custom layer\n",
"\n",
"encoder = models.Model(x, [mean, logsigma, sample])\n",
"\n",
"encoded_inputs = layers.Input(shape=(encoded_dim,))\n",
"h2 = layers.Dense(intermediate_dim, activation='relu')(encoded_inputs) # hidden decoding layer\n",
"outputs = layers.Dense(original_dim, activation='sigmoid')(h2) # output layer\n",
"\n",
"decoder = models.Model(encoded_inputs, outputs)\n",
"vae = models.Model(x, decoder(encoder(x)[2])) # compose to get full variational autoencoder"]},{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"vae.compile(optimizer='adam', loss = vae_loss)\n",
"vae.fit(x_train, x_train,  \n",
"        epochs = epochs, \n",
"        batch_size = batch_size)"]},{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"def random_image():\n",
"    return decoder.predict(\n",
"      np.random.randn(encoded_dim)\n",
"      [np.newaxis, :]).reshape((28,28))\n",
"\n",
"plt.figure(figsize=(10,10))\n",
"for i in range(25):\n",
"    plt.subplot(5,5,i+1)\n",
"    plt.imshow(random_image())\n",
"    plt.axis('off')\n",
"plt.show()"]}]}